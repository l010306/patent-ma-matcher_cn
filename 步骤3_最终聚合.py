# -*- coding: utf-8 -*-
"""
æœ€ç»ˆæ•°æ®èšåˆä¼˜åŒ–ç‰ˆï¼ˆæ­¥éª¤3ï¼‰
============================
åŠŸèƒ½ï¼šä½¿ç”¨è¶…çº§å­—å…¸å°†ä¸“åˆ©æ•°æ®èšåˆåˆ°final_outcomeæ–‡ä»¶

æ”¹è¿›ç‚¹ï¼š
1. è¯¦ç»†çš„æ—¥å¿—è®°å½•
2. è¿›åº¦æ˜¾ç¤º
3. æ•°æ®éªŒè¯
4. å‘é‡åŒ–å‘æ˜äººç»Ÿè®¡ï¼ˆéŸ©è¯­è¦æ±‚ï¼‰
5. è‡ªåŠ¨å¤„ç†ç¼ºå¤±åˆ—
"""

import pandas as pd
import numpy as np
import pickle
import logging
from datetime import datetime
from tqdm import tqdm

# ==========================================
# é…ç½®æ—¥å¿—
# ==========================================
# ç¡®ä¿logsæ–‡ä»¶å¤¹å­˜åœ¨
import os
if not os.path.exists('logs'):
    os.makedirs('logs')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/aggregation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# 1. è·¯å¾„é…ç½®
# ==========================================
# è¶…çº§å­—å…¸è·¯å¾„
DICT_PATH = '/Users/lidachuan/Desktop/Patent Data/Master_Company_Dictionary.pkl'

# ä¸»æ•°æ®åº“æ¨¡æ¿è·¯å¾„
FINAL_OUTCOME_PATH = '/Users/lidachuan/Desktop/Patent Data/final_outcome.xlsx'

# ä¸“åˆ©æ•°æ®åº“è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å•ä¸ªCSVæˆ–ç›®å½•ï¼‰
PATENT_DB_PATH = '/Users/lidachuan/Desktop/Patent Data/1993-1997/patent_database.csv'

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
OUTPUT_PATH = '/Users/lidachuan/Desktop/Patent Data/final_outcome_1993_1997_COMPLETE.xlsx'

# ==========================================
# 2. å‘æ˜äººç»Ÿè®¡å‡½æ•°ï¼ˆç¬¦åˆéŸ©è¯­è¦æ±‚ï¼‰
# ==========================================

def calculate_inventor_count_vectorized(df, inventor_cols):
    """
    æŒ‰ç…§éŸ©è¯­è¦æ±‚ï¼šå– inventors åˆ—å’Œåå­—åˆ—è®¡æ•°çš„è¾ƒå¤§å€¼
    ä½¿ç”¨å‘é‡åŒ–æ“ä½œæå‡æ€§èƒ½
    """
    # 1. ä» inventors åˆ—è·å–æ•°å€¼
    num_from_column = pd.to_numeric(df['inventors'], errors='coerce').fillna(0)
    
    # 2. ä»åå­—åˆ—è®¡æ•°ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
    num_from_names = df[inventor_cols].notna().sum(axis=1)
    
    # 3. å–ä¸¤è€…ä¸­çš„è¾ƒå¤§å€¼ï¼ˆç¬¦åˆéŸ©è¯­è¦æ±‚ï¼‰
    return np.maximum(num_from_column, num_from_names)

# ==========================================
# 3. ä¸»å¤„ç†å‡½æ•°
# ==========================================

def load_master_dictionary():
    """åŠ è½½è¶…çº§å­—å…¸"""
    logger.info("æ­¥éª¤ 1/6: åŠ è½½è¶…çº§å­—å…¸...")
    try:
        with open(DICT_PATH, 'rb') as f:
            master_dict = pickle.load(f)
        logger.info(f"   âœ… å­—å…¸åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(master_dict):,} ä¸ªæ˜ å°„å…³ç³»")
        return master_dict
    except FileNotFoundError:
        logger.error(f"   âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°å­—å…¸æ–‡ä»¶ {DICT_PATH}")
        logger.error("   è¯·å…ˆè¿è¡Œæ­¥éª¤2ï¼ˆè¶…çº§å­—å…¸æ„å»ºï¼‰")
        raise


def load_main_database():
    """åŠ è½½ä¸»æ•°æ®åº“æ¨¡æ¿"""
    logger.info("\næ­¥éª¤ 2/6: åŠ è½½ä¸»æ•°æ®åº“æ¨¡æ¿...")
    try:
        df_main = pd.read_excel(FINAL_OUTCOME_PATH)
        df_main.drop_duplicates(subset=['acquiror_name'], keep='first', inplace=True)
        logger.info(f"   âœ… æ¨¡æ¿åŠ è½½æˆåŠŸï¼Œå…± {len(df_main):,} å®¶å…¬å¸")
        return df_main
    except FileNotFoundError:
        logger.error(f"   âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶ {FINAL_OUTCOME_PATH}")
        raise


def load_patent_database():
    """åŠ è½½ä¸“åˆ©æ•°æ®åº“"""
    logger.info("\næ­¥éª¤ 3/6: åŠ è½½ä¸“åˆ©æ•°æ®åº“...")
    try:
        df_patent = pd.read_csv(PATENT_DB_PATH, low_memory=False)
        original_count = len(df_patent)
        
        # å»é™¤assigneeä¸ºç©ºçš„è¡Œ
        df_patent.dropna(subset=['assignee'], inplace=True)
        logger.info(f"   âœ… ä¸“åˆ©æ•°æ®åŠ è½½å®Œæˆ: {len(df_patent):,} æ¡æœ‰æ•ˆè®°å½•ï¼ˆåŸå§‹ {original_count:,}ï¼‰")
        return df_patent
    except FileNotFoundError:
        logger.error(f"   âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ä¸“åˆ©æ•°æ®æ–‡ä»¶ {PATENT_DB_PATH}")
        raise


def process_patent_data(df_patent, master_dict):
    """å¤„ç†ä¸“åˆ©æ•°æ®ï¼šåº”ç”¨å­—å…¸æ˜ å°„å’Œç»Ÿè®¡å‘æ˜äºº"""
    logger.info("\næ­¥éª¤ 4/6: å¤„ç†ä¸“åˆ©æ•°æ®...")
    
    # åº”ç”¨æ˜ å°„
    logger.info("   åº”ç”¨å­—å…¸æ˜ å°„...")
    df_patent['assignee_stripped'] = df_patent['assignee'].astype(str).str.strip()
    df_patent['Matched_Acquiror'] = df_patent['assignee_stripped'].map(master_dict)
    
    # ç»Ÿè®¡åŒ¹é…ç‡
    matched_count = df_patent['Matched_Acquiror'].notna().sum()
    match_rate = matched_count / len(df_patent) * 100
    logger.info(f"   âœ… æ˜ å°„å®Œæˆ: {matched_count:,} / {len(df_patent):,} ({match_rate:.2f}%)")
    
    # åªä¿ç•™åŒ¹é…æˆåŠŸçš„
    df_matched = df_patent.dropna(subset=['Matched_Acquiror']).copy()
    
    # æ¸…æ´—å¹´ä»½
    logger.info("   æ¸…æ´—å¹´ä»½æ•°æ®...")
    df_matched['application_year'] = pd.to_numeric(df_matched['application_year'], errors='coerce')
    df_matched = df_matched.dropna(subset=['application_year'])
    df_matched['application_year'] = df_matched['application_year'].astype(int)
    
    # ç»Ÿè®¡å‘æ˜äººæ•°é‡ï¼ˆæŒ‰éŸ©è¯­è¦æ±‚ï¼‰
    logger.info("   è®¡ç®—å‘æ˜äººæ•°é‡...")
    inventor_name_cols = [f'inventor_name{i}' for i in range(1, 11)]
    
    # ç¡®ä¿åˆ—å­˜åœ¨
    for col in inventor_name_cols:
        if col not in df_matched.columns:
            df_matched[col] = np.nan
    
    df_matched['final_inventor_count'] = calculate_inventor_count_vectorized(
        df_matched, 
        inventor_name_cols
    )
    
    logger.info(f"   âœ… å¤„ç†å®Œæˆï¼Œå¹³å‡æ¯ä¸“åˆ© {df_matched['final_inventor_count'].mean():.2f} ä½å‘æ˜äºº")
    
    return df_matched


def aggregate_data(df_matched):
    """èšåˆæ•°æ®ï¼šæŒ‰å…¬å¸å’Œå¹´ä»½ç»Ÿè®¡"""
    logger.info("\næ­¥éª¤ 5/6: èšåˆæ•°æ®...")
    
    # æŒ‰å…¬å¸å’Œå¹´ä»½åˆ†ç»„
    logger.info("   æŒ‰å…¬å¸å’Œå¹´ä»½åˆ†ç»„ç»Ÿè®¡...")
    df_grouped = df_matched.groupby(['Matched_Acquiror', 'application_year']).agg({
        'assignee': 'count',  # ä¸“åˆ©æ•°é‡
        'final_inventor_count': 'sum'  # å‘æ˜äººæ€»æ•°
    }).reset_index()
    
    # é€è§†è¡¨ï¼šä¸“åˆ©æ•°é‡
    logger.info("   ç”Ÿæˆä¸“åˆ©æ•°é‡é€è§†è¡¨...")
    pivot_patent = df_grouped.pivot(
        index='Matched_Acquiror', 
        columns='application_year', 
        values='assignee'
    )
    pivot_patent.columns = [f'patent_{int(col)}' for col in pivot_patent.columns]
    
    # é€è§†è¡¨ï¼šå‘æ˜äººæ•°é‡
    logger.info("   ç”Ÿæˆå‘æ˜äººæ•°é‡é€è§†è¡¨...")
    pivot_inventor = df_grouped.pivot(
        index='Matched_Acquiror', 
        columns='application_year', 
        values='final_inventor_count'
    )
    pivot_inventor.columns = [f'patent_inventor_{int(col)}' for col in pivot_inventor.columns]
    
    # åˆå¹¶é€è§†è¡¨
    df_stats = pd.concat([pivot_patent, pivot_inventor], axis=1).reset_index()
    df_stats.rename(columns={'Matched_Acquiror': 'acquiror_name'}, inplace=True)
    
    logger.info(f"   âœ… èšåˆå®Œæˆï¼Œæ¶µç›– {len(df_stats)} å®¶å…¬å¸")
    logger.info(f"   å¹´ä»½èŒƒå›´: {pivot_patent.columns.tolist()[:3]}...{pivot_patent.columns.tolist()[-3:]}")
    
    # æ”¶é›†å…¬å¸åˆ«å
    logger.info("   æ”¶é›†å…¬å¸åˆ«å...")
    df_names = df_matched.groupby('Matched_Acquiror')['assignee'].apply(
        lambda x: list(set(x))
    ).reset_index()
    
    # å±•å¼€åˆ«ååˆ—è¡¨
    max_len = df_names['assignee'].apply(len).max() if not df_names.empty else 0
    name_cols = ['patent_name'] + [f'patent_name_{i}' for i in range(1, max_len)]
    names_expanded = pd.DataFrame(df_names['assignee'].tolist(), index=df_names.index)
    names_expanded = names_expanded.iloc[:, :len(name_cols)]
    names_expanded.columns = name_cols[:names_expanded.shape[1]]
    
    df_names = pd.concat([df_names[['Matched_Acquiror']], names_expanded], axis=1)
    df_names.rename(columns={'Matched_Acquiror': 'acquiror_name'}, inplace=True)
    
    return df_stats, df_names


def merge_to_final_outcome(df_main, df_stats, df_names):
    """åˆå¹¶åˆ°æœ€ç»ˆæ–‡ä»¶"""
    logger.info("\næ­¥éª¤ 6/6: åˆå¹¶åˆ°æœ€ç»ˆæ–‡ä»¶...")
    
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§åˆ—
    logger.info("   æ¸…ç†æ—§çš„ç»Ÿè®¡åˆ—...")
    cols_to_remove = [c for c in df_main.columns 
                     if c.startswith('patent_') or c.startswith('patent_inventor_')]
    if cols_to_remove:
        df_main = df_main.drop(columns=cols_to_remove, errors='ignore')
        logger.info(f"   ç§»é™¤äº† {len(cols_to_remove)} ä¸ªæ—§åˆ—")
    
    # åˆå¹¶ç»Ÿè®¡æ•°æ®
    logger.info("   åˆå¹¶ç»Ÿè®¡æ•°æ®...")
    df_final = pd.merge(df_main, df_stats, on='acquiror_name', how='left')
    
    # åˆå¹¶åˆ«åæ•°æ®
    logger.info("   åˆå¹¶åˆ«åæ•°æ®...")
    df_final = pd.merge(df_final, df_names, on='acquiror_name', how='left')
    
    # å¡«å…… NaN ä¸º 0ï¼ˆä»…æ•°å€¼åˆ—ï¼‰
    stat_cols = [c for c in df_final.columns 
                if (c.startswith('patent_') or c.startswith('patent_inventor_')) 
                and 'name' not in c]
    df_final[stat_cols] = df_final[stat_cols].fillna(0).astype(int)
    
    logger.info(f"   âœ… åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆæ–‡ä»¶å…± {len(df_final)} è¡Œ")
    
    # ç»Ÿè®¡æœ‰æ•°æ®çš„å…¬å¸
    companies_with_patents = (df_final[stat_cols].sum(axis=1) > 0).sum()
    logger.info(f"   å…¶ä¸­ {companies_with_patents} å®¶å…¬å¸æœ‰ä¸“åˆ©æ•°æ®")
    
    return df_final


def save_output(df_final):
    """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
    logger.info("\nä¿å­˜ç»“æœ...")
    df_final.to_excel(OUTPUT_PATH, index=False)
    logger.info(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_PATH}")


# ==========================================
# 4. ä¸»æ‰§è¡Œæµç¨‹
# ==========================================

def main():
    start_time = datetime.now()
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹æœ€ç»ˆæ•°æ®èšåˆæµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    logger.info("=" * 60)
    
    try:
        # æ­¥éª¤1: åŠ è½½å­—å…¸
        master_dict = load_master_dictionary()
        
        # æ­¥éª¤2: åŠ è½½ä¸»æ•°æ®åº“
        df_main = load_main_database()
        
        # æ­¥éª¤3: åŠ è½½ä¸“åˆ©æ•°æ®
        df_patent = load_patent_database()
        
        # æ­¥éª¤4: å¤„ç†ä¸“åˆ©æ•°æ®
        df_matched = process_patent_data(df_patent, master_dict)
        
        # æ­¥éª¤5: èšåˆæ•°æ®
        df_stats, df_names = aggregate_data(df_matched)
        
        # æ­¥éª¤6: åˆå¹¶åˆ°æœ€ç»ˆæ–‡ä»¶
        df_final = merge_to_final_outcome(df_main, df_stats, df_names)
        
        # ä¿å­˜ç»“æœ
        save_output(df_final)
        
        # å®Œæˆæ‘˜è¦
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info("\n" + "=" * 60)
        logger.info("å¤„ç†å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"â±  æ€»è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {len(df_patent) / duration:.0f} æ¡/ç§’")
        logger.info(f"\nâœ… ä¸‹ä¸€æ­¥: è¿è¡Œæ­¥éª¤4ï¼ˆCompustatåŒ¹é…ï¼‰")
        
        return True
        
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
