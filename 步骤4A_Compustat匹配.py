# -*- coding: utf-8 -*-
"""
CompustatåŒ¹é… - æ­¥éª¤4Aï¼šç”ŸæˆéªŒè¯æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
================================================
åŠŸèƒ½ï¼šå°†final_outcomeä¸CompustatåŒ¹é…ï¼Œç”Ÿæˆäººå·¥éªŒè¯æ–‡ä»¶

æ”¹è¿›ç‚¹ï¼š
1. ä½¿ç”¨rapidfuzzæ›¿ä»£thefuzzï¼ˆæ›´å¿«ï¼‰
2. è¯¦ç»†çš„æ—¥å¿—å’Œè¿›åº¦æ˜¾ç¤º
3. æ”¹è¿›çš„æ¸…æ´—å‡½æ•°ï¼ˆä¸æ­¥éª¤1ä¸€è‡´ï¼‰
"""

import pandas as pd
import re
from rapidfuzz import process, fuzz
from tqdm import tqdm
import logging
from datetime import datetime

# ==========================================
# é…ç½®æ—¥å¿—
# ==========================================
# ç¡®ä¿logsæ–‡ä»¶å¤¹å­˜åœ¨
import os
if not os.path.exists('logs'):
    os.makedirs('logs')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/compustat_match_4A_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# 1. è·¯å¾„ä¸é…ç½®
# ==========================================
PATH_MA = "/Users/lidachuan/Desktop/Patent Data/final_outcome_1993_1997_COMPLETE.xlsx"
PATH_COMPUSTAT = "/Users/lidachuan/Desktop/Patent Data/compustat_19802025.csv"
OUTPUT_VERIFICATION = "/Users/lidachuan/Desktop/Patent Data/company_match_verification.xlsx"

FUZZY_THRESHOLD = 90  # æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼

# ==========================================
# 2. æ¸…æ´—å‡½æ•°ï¼ˆä¸æ­¥éª¤1ä¿æŒä¸€è‡´ï¼‰
# ==========================================

def clean_company_name(name):
    """ä¼˜åŒ–çš„æ ‡å‡†åŒ–æ¸…æ´—å‡½æ•°"""
    if pd.isna(name) or not isinstance(name, str):
        return ""
    
    name = str(name).upper().strip()
    
    # 1. å¤„ç†å¸¸è§ç¬¦å·
    name = name.replace('&', ' AND ')
    name = name.replace('-', ' ')
    name = name.replace("'", '')
    
    # 2. æ‰©å±•å¸¸è§ç¼©å†™
    abbreviations = {
        r'\bINTL\b': 'INTERNATIONAL',
        r'\bNATL\b': 'NATIONAL',
        r'\bCORP\b': 'CORPORATION',
        r'\bINC\b': 'INCORPORATED',
        r'\bMFG\b': 'MANUFACTURING',
        r'\bTECH\b': 'TECHNOLOGY',
        r'\bSYS\b': 'SYSTEMS',
    }
    for abbr, full in abbreviations.items():
        name = re.sub(abbr, full, name)
    
    # 3. å»é™¤åç¼€
    suffixes_priority = [
        r'\bINCORPORATED\b', r'\bCORPORATION\b', r'\bCOMPANY\b',
        r'\bLIMITED\b', r'\bGROUP\b',
        r'\bCORP\.?\b', r'\bINC\.?\b', r'\bLTD\.?\b', 
        r'\bCO\.?\b', r'\bL\.L\.C\.?\b', r'\bPLC\.?\b',
        r'\bLLC\b', r'\bS\.A\\.b', r'\bNV\b', r'\bGMBH\b',
        r'\bSA\b', r'\bAG\b', r'\bKK\b'
    ]
    
    for suffix in suffixes_priority:
        name = re.sub(suffix, '', name, flags=re.IGNORECASE)
    
    # 4. å»é™¤æ ‡ç‚¹
    name = re.sub(r'[^A-Z0-9\s]', ' ', name)
    
    # 5. åˆå¹¶ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name).strip()
    
    return name


# ==========================================
# 3. ä¸»å¤„ç†å‡½æ•°
# ==========================================

def main():
    start_time = datetime.now()
    
    logger.info("=" * 60)
    logger.info("CompustatåŒ¹é… - æ­¥éª¤4Aï¼šç”ŸæˆéªŒè¯æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    logger.info("=" * 60)
    
    # ========== æ­¥éª¤1: åŠ è½½æ•°æ® ==========
    logger.info("\næ­¥éª¤ 1/4: åŠ è½½æ•°æ®...")
    
    # è¯»å– M&A æ•°æ®
    logger.info("   åŠ è½½ M&A æ•°æ®...")
    try:
        df_ma = pd.read_excel(PATH_MA)
        logger.info(f"   âœ… M&A æ•°æ®åŠ è½½æˆåŠŸ: {len(df_ma):,} è¡Œ")
    except Exception as e:
        logger.error(f"   âŒ è¯»å–å¤±è´¥: {e}")
        return False
    
    # è¿‡æ»¤ï¼šåªå¤„ç†æœ‰ patent_name çš„è¡Œ
    df_ma_target = df_ma[df_ma['patent_name'].notna()].copy()
    logger.info(f"   è¿‡æ»¤ patent_name éç©º: {len(df_ma_target):,} è¡Œå¾…åŒ¹é…")
    
    # è¯»å– Compustat æ•°æ®ï¼ˆåªè¯»å– conm åˆ—ä»¥èŠ‚çœå†…å­˜ï¼‰
    logger.info("   åŠ è½½ Compustat æ•°æ®ï¼ˆä»…å…¬å¸ååˆ—ï¼‰...")
    try:
        # ç­–ç•¥ï¼šå¤§æ–‡ä»¶åªè¯»å–éœ€è¦çš„åˆ—ï¼ˆconmï¼‰
        df_comp = pd.read_csv(PATH_COMPUSTAT, usecols=['conm'], low_memory=False)
        logger.info(f"   âœ… Compustat æ•°æ®åŠ è½½æˆåŠŸ: {len(df_comp):,} è¡Œ")
    except ValueError:
        # å¦‚æœåˆ—åä¸åŒ¹é…ï¼Œå°è¯•å…¨é‡è¯»å–ï¼ˆä½†å¯èƒ½å¾ˆæ…¢ï¼‰
        logger.warning("   åˆ—å 'conm' æœªæ‰¾åˆ°ï¼Œå°è¯•å…¨é‡è¯»å–...")
        df_comp = pd.read_csv(PATH_COMPUSTAT, low_memory=False)
        logger.info(f"   âœ… Compustat æ•°æ®åŠ è½½æˆåŠŸï¼ˆå…¨é‡ï¼‰: {len(df_comp):,} è¡Œ")
    except Exception as e:
        logger.error(f"   âŒ è¯»å–å¤±è´¥: {e}")
        return False
    
    # ========== æ­¥éª¤2: æ¸…æ´—æ•°æ® ==========
    logger.info("\næ­¥éª¤ 2/4: æ¸…æ´—å…¬å¸åç§°...")
    
    # æ¸…æ´— M&A çš„ acquiror_name
    df_ma_target['clean_acquiror'] = df_ma_target['acquiror_name'].apply(clean_company_name)
    
    # æ¸…æ´— Compustat çš„ conm
    df_comp['clean_conm'] = df_comp['conm'].apply(clean_company_name)
    
    # åˆ›å»º Compustat æŸ¥æ‰¾é›†åˆ
    compustat_unique = df_comp[df_comp['clean_conm'] != ""][['conm', 'clean_conm']].drop_duplicates(subset=['clean_conm'])
    compustat_clean_set = set(compustat_unique['clean_conm'])
    compustat_clean_list = list(compustat_unique['clean_conm'])
    
    logger.info(f"   âœ… Compustat å”¯ä¸€å…¬å¸å: {len(compustat_clean_list):,}")
    
    # ========== æ­¥éª¤3: æ‰§è¡ŒåŒ¹é… ==========
    logger.info("\næ­¥éª¤ 3/4: æ‰§è¡ŒåŒ¹é…...")
    
    strict_res = []
    fuzzy_res = []
    unmatched_rows = []
    
    # 3.1 ä¸¥æ ¼åŒ¹é…
    logger.info("   é˜¶æ®µ 3.1: ç²¾ç¡®åŒ¹é…...")
    for idx, row in df_ma_target.iterrows():
        acquiror_orig = row['acquiror_name']
        acquiror_clean = row['clean_acquiror']
        
        if not acquiror_clean:
            continue
        
        if acquiror_clean in compustat_clean_set:
            strict_res.append({
                'Acquiror_Original': acquiror_orig,
                'Acquiror_Clean': acquiror_clean,
                'Matched_Compustat_Clean': acquiror_clean,
                'Match_Type': 'Strict',
                'Score': 100
            })
        else:
            unmatched_rows.append(row)
    
    logger.info(f"   âœ… ç²¾ç¡®åŒ¹é…: {len(strict_res)} æ¡")
    logger.info(f"   å¾…æ¨¡ç³ŠåŒ¹é…: {len(unmatched_rows)} æ¡")
    
    # 3.2 æ¨¡ç³ŠåŒ¹é…
    if len(unmatched_rows) > 0:
        logger.info(f"   é˜¶æ®µ 3.2: æ¨¡ç³ŠåŒ¹é… (é˜ˆå€¼ {FUZZY_THRESHOLD})...")
        
        for row in tqdm(unmatched_rows, desc="   åŒ¹é…è¿›åº¦"):
            acquiror_orig = row['acquiror_name']
            acquiror_clean = row['clean_acquiror']
            
            match_result = process.extractOne(
                acquiror_clean, 
                compustat_clean_list, 
                scorer=fuzz.token_set_ratio,
                score_cutoff=FUZZY_THRESHOLD
            )
            
            if match_result:
                match_name, score, _ = match_result
                fuzzy_res.append({
                    'Acquiror_Original': acquiror_orig,
                    'Acquiror_Clean': acquiror_clean,
                    'Matched_Compustat_Clean': match_name,
                    'Match_Type': 'Fuzzy',
                    'Score': score
                })
        
        logger.info(f"   âœ… æ¨¡ç³ŠåŒ¹é…: {len(fuzzy_res)} æ¡")
    
    # ========== æ­¥éª¤4: ç”ŸæˆéªŒè¯æ–‡ä»¶ ==========
    logger.info("\næ­¥éª¤ 4/4: ç”Ÿæˆäººå·¥éªŒè¯æ–‡ä»¶...")
    
    # åˆå¹¶ç»“æœ
    df_strict = pd.DataFrame(strict_res)
    df_fuzzy = pd.DataFrame(fuzzy_res)
    df_all_matches = pd.concat([df_strict, df_fuzzy], ignore_index=True)
    
    if df_all_matches.empty:
        logger.warning("   âš ï¸  æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ç»“æœ")
        return False
    
    # æ‰¾å› Compustat åŸå§‹åç§°
    clean_to_original_map = dict(zip(compustat_unique['clean_conm'], compustat_unique['conm']))
    df_all_matches['Matched_Compustat_Original'] = df_all_matches['Matched_Compustat_Clean'].map(clean_to_original_map)
    
    # é€‰æ‹©è¾“å‡ºåˆ—
    output_columns = [
        'Acquiror_Original',
        'Matched_Compustat_Original',
        'Match_Type',
        'Score',
        'Acquiror_Clean',
        'Matched_Compustat_Clean'
    ]
    
    df_verify = df_all_matches[output_columns].copy()
    
    # æ’åºï¼šFuzzy åœ¨å‰ï¼Œåˆ†æ•°ä½çš„ä¼˜å…ˆå®¡æŸ¥
    df_verify.sort_values(by=['Match_Type', 'Score'], ascending=[True, True], inplace=True)
    
    # å¯¼å‡º
    df_verify.to_excel(OUTPUT_VERIFICATION, index=False)
    
    # ========== å®Œæˆæ‘˜è¦ ==========
    duration = (datetime.now() - start_time).total_seconds()
    
    logger.info("\n" + "=" * 60)
    logger.info("æ­¥éª¤4A å®Œæˆï¼")
    logger.info("=" * 60)
    logger.info(f"â±  æ€»è€—æ—¶: {duration:.2f} ç§’")
    logger.info(f"ğŸ“Š åŒ¹é…ç»“æœ:")
    logger.info(f"   - ç²¾ç¡®åŒ¹é…: {len(strict_res)}")
    logger.info(f"   - æ¨¡ç³ŠåŒ¹é…: {len(fuzzy_res)}")
    logger.info(f"   - æ€»è®¡: {len(df_verify):,} å¯¹")
    logger.info(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"   {OUTPUT_VERIFICATION}")
    logger.info(f"\nâš ï¸  ä¸‹ä¸€æ­¥ï¼ˆé‡è¦ï¼‰:")
    logger.info(f"   1. æ‰“å¼€ {OUTPUT_VERIFICATION}")
    logger.info(f"   2. äººå·¥å®¡æ ¸ï¼Œåˆ é™¤é”™è¯¯çš„åŒ¹é…è¡Œ")
    logger.info(f"   3. ä¿å­˜æ–‡ä»¶ï¼ˆä¿æŒæ–‡ä»¶åä¸å˜ï¼‰")
    logger.info(f"   4. è¿è¡Œæ­¥éª¤4Bï¼ˆCompustatåŒ¹é…_æ­¥éª¤4B_ä¼˜åŒ–ç‰ˆ.pyï¼‰")
    logger.info("=" * 60)
    
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
